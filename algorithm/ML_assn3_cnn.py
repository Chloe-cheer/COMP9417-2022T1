# -*- coding: utf-8 -*-
"""MLproj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBfvNizBpxdYqOpQjhNgueqIp8kBAWdV
"""

import torch
import sklearn.metrics as metrics
import numpy as np
import sys
import matplotlib.pyplot as plt
import statistics
# import tensorflow as tf
import torch.optim as optim

# from google.colab import drive
# !unzip /content/drive/MyDrive/Colab\ Notebooks/MLproj/X_train_scaled.zip -d "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cp ./drive/MyDrive/Colab\ Notebooks/MLproj/y_train.npy ./

X_train = np.load("X_train_scaled_00.npy")
print(X_train.shape)

for i in range(1, 10):
  X_train = np.concatenate((X_train, np.load("X_train_scaled_0{}.npy".format(str(i)))), axis=0)
for i in range(10, 18):
  X_train = np.concatenate((X_train, np.load("X_train_scaled_{}.npy".format(str(i)))), axis=0)

X_test = np.load("X_train_scaled_18.npy")
X_test = np.concatenate((X_test, np.load("X_train_scaled_19.npy")), axis=0)
print(X_train.shape)
print(X_test.shape)
y = np.load("y_train.npy")
y_train = y[0:X_train.shape[0]]
y_test = y[X_train.shape[0]:]
print(y_train.shape)
print(y_test.shape)

train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))
train_dataloader = DataLoader(train_dataset, shuffle=True)

test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))
test_dataloader = DataLoader(test_dataset, shuffle=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):
        super(Block, self).__init__()
        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)
        self.bn3 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.identity_downsample = identity_downsample

    def forward(self, x):
        # saving weights at beginning to reapply at end of block
        identity = x

        # main block architecture - 2 conv layers with batch norm and relu
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)

        # if there is a size mismatch, downsample identity before readding
        if self.identity_downsample is not None:
            identity = self.identity_downsample(identity)

        # reapply weights from beginning of block
        x += identity
        x = self.relu(x)
        return x


class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        block=Block
        image_channels=1
        num_classes=5
        layers = [2, 2, 2]
        self.in_channels = 64

        # input layer
        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=5, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNetLayers
        self.layer1 = self.make_layers(block, layers[0], intermediate_channels=64, stride=1)
        self.layer2 = self.make_layers(block, layers[1], intermediate_channels=128, stride=2)
        self.layer3 = self.make_layers(block, layers[2], intermediate_channels=256, stride=2)

        # output layer
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def forward(self, x):
      # input layer fwding
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

      # ResNet layers
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

      # output layer
        x = self.avgpool(x)
        x = x.reshape(x.shape[0], -1)
        x = self.fc(x)
        return x

    # function to make resnet layers based on given parameters
    def make_layers(self, block, num_residual_blocks, intermediate_channels, stride):
        layers = []

        # used if size of identity is incompatible with output of block
        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels, kernel_size=1, stride=stride),
                                            nn.BatchNorm2d(intermediate_channels))
        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))
        self.in_channels = intermediate_channels # 256
        for i in range(num_residual_blocks - 1):
            layers.append(block(self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again
        return nn.Sequential(*layers)

net = Network()
model = net.to(device)
print(model)

optimizer = optim.Adam(net.parameters(), lr = 0.001)

loss_func = nn.CrossEntropyLoss()

train_val_split = 0.8
batch_size = 1
epochs = 40

print("Start training...")
for epoch in range(1,epochs+1):
    total_loss = 0
    total_images = 0
    total_correct = 0
    for batch in train_dataloader:           # Load batch
        images, labels = batch
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        images = images.to(device, dtype=torch.float)
        labels = labels.type(torch.LongTensor)
        labels = labels.to(device)

        preds = model(images)             # Process batch

        loss = loss_func(preds, labels) # Calculate loss

        optimizer.zero_grad()
        loss.backward()                 # Calculate gradients
        optimizer.step()                # Update weights

        output = preds.argmax(dim=1)

        total_loss += loss.item()
        total_images += labels.size(0)
        total_correct += output.eq(labels).sum().item()

    model_accuracy = total_correct / total_images * 100

    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(
            epoch, total_loss, total_images, model_accuracy), end='')


    print()

    if epoch % 10 == 0:
        torch.save(net.state_dict(),'checkModel.pth')
        print("   Model saved to checkModel.pth")

    if model_accuracy > 99:
      break
    sys.stdout.flush()

torch.save(net.state_dict(),'savedModel.pth')
print("   Model saved to savedModel.pth")

# Function to test the model
def test():
    # Load the model that we saved at the end of the training loop
    model = Network()
    path = "savedModel.pth"
    model.load_state_dict(torch.load(path))
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using {device} device")
    model.to(device)
    running_accuracy = 0
    total = 0

    with torch.no_grad():
        for data in test_dataloader:
            images, outputs = data
            outputs = outputs.to(device, dtype=torch.float32)
            images = images.to(device, dtype=torch.float)
            predicted_outputs = model(images)
            _, predicted = torch.max(predicted_outputs, 1)
            total += outputs.size(0)
            running_accuracy += (predicted == outputs).sum().item()

        print('Accuracy of the model based on the test set of the inputs is: %d %%' % (100 * running_accuracy / total))

test()
