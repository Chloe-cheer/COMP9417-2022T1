# -*- coding: utf-8 -*-
"""MLproj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBfvNizBpxdYqOpQjhNgueqIp8kBAWdV
"""

from typing import Any, Dict, List, Tuple
import torch
import sklearn.metrics as metrics
import numpy as np
import sys
import matplotlib.pyplot as plt
import statistics
# import tensorflow as tf
import torch.optim as optim
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
import time
import datetime

from modules.unet import UNet

# from algorithm.modules import UNet

# from google.colab import drive
# !unzip /content/drive/MyDrive/Colab\ Notebooks/MLproj/X_train_scaled.zip -d "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cp ./drive/MyDrive/Colab\ Notebooks/MLproj/y_train.npy ./

# CONFIGS
base_path = "../input/"

x_train_path = base_path+"X_train_chunks/"
y_train_path = base_path+"y_train.npy"

split_ratio = "8:0:2" # train:val:test
batch_size = 4

split_ratio_lst = list(map(lambda x: int(x), split_ratio.split(":")))
split_ratio_lst_total = sum(split_ratio_lst)
data_size = 20
split_slicing_indices = [int(data_size*(r/split_ratio_lst_total)) for r in split_ratio_lst]
split_slicing_ranges: Dict[str, Tuple[int]] = {
    "train": (0, split_slicing_indices[0]), 
    "val": (split_slicing_indices[0], sum(split_slicing_indices[:2])),
    "test": (sum(split_slicing_indices[:2]), sum(split_slicing_indices)),
}
print(f"split_slicing_ranges are {split_slicing_ranges}")

def load_X_by_ratio(dataset_type: str) -> Any:
    X = None
    for i in range(*split_slicing_ranges[dataset_type]):
        if X is None:
            X = np.load(x_train_path+f"X_train_scaled_{i:02d}.npy")
        else:
            X = np.concatenate((X, np.load(x_train_path+f"X_train_scaled_{i:02d}.npy")), axis=0)
    return X

def load_all_y_by_ratio(shape_values: List[int]) -> Tuple[Any]:
    y = np.load(y_train_path)
    y_train = y[:shape_values[0]]
    y_val = y[shape_values[0]:sum(shape_values[:2])]
    y_test = y[sum(shape_values[:2]):]
    return y_train, y_val, y_test

X_train = load_X_by_ratio("train")
X_val = load_X_by_ratio("val")
X_test = load_X_by_ratio("test")

validation = False
if X_val is not None:
    validation = True

print(f"X_train.shape: {X_train.shape}") 
if validation:
    print(f"X_val.shape: {X_val.shape}") 
print(f"X_test.shape: {X_test.shape}") 

y_train, y_val, y_test = load_all_y_by_ratio([X_train.shape[0], X_val.shape[0] if validation else 0, X_test.shape[0]])
print(f"y_train.shape: {y_train.shape}") 
if validation:
    print(f"y_val.shape: {y_val.shape}") 
print(f"y_test.shape: {y_test.shape}") 

# from collections import Counter
# print(Counter(y_train))
# print(Counter(y_val))
# # print(Counter(np.concatenate((y_train, y_test), axis=0)))
# exit()

train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)

if validation:
    val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))
    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)

test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))
test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):
        super(Block, self).__init__()
        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)
        self.bn3 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.identity_downsample = identity_downsample

    def forward(self, x):
        # saving weights at beginning to reapply at end of block
        identity = x

        # main block architecture - 2 conv layers with batch norm and relu
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)

        # if there is a size mismatch, downsample identity before readding
        if self.identity_downsample is not None:
            identity = self.identity_downsample(identity)

        # reapply weights from beginning of block
        x += identity
        x = self.relu(x)
        return x


class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        block=Block
        image_channels=1
        num_classes=5
        layers = [2, 2, 2]
        self.in_channels = 64

        # input layer
        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=5, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNetLayers
        self.layer1 = self.make_layers(block, layers[0], intermediate_channels=64, stride=1)
        self.layer2 = self.make_layers(block, layers[1], intermediate_channels=128, stride=2)
        self.layer3 = self.make_layers(block, layers[2], intermediate_channels=256, stride=2)

        # output layer
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def forward(self, x):
      # input layer fwding
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

      # ResNet layers
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

      # output layer
        x = self.avgpool(x)
        x = x.reshape(x.shape[0], -1)
        x = self.fc(x)
        return x

    # function to make resnet layers based on given parameters
    def make_layers(self, block, num_residual_blocks, intermediate_channels, stride):
        layers = []

        # used if size of identity is incompatible with output of block
        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels, kernel_size=1, stride=stride),
                                            nn.BatchNorm2d(intermediate_channels))
        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))
        self.in_channels = intermediate_channels # 256
        for i in range(num_residual_blocks - 1):
            layers.append(block(self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again
        return nn.Sequential(*layers)

net = Network()
# net = UNet(n_channels=1, n_classes=4)
model = net.to(device)
print(model)

optimizer = optim.Adam(net.parameters(), lr = 0.001)

loss_func = nn.CrossEntropyLoss()

epochs = 30

start = time.time()
best_accuracy = 0.0 

print("Start training...")
for epoch in range(1,epochs+1):
    total_train_loss = 0
    total_val_loss = 0
    total_images = 0
    total_correct = 0

    # Training
    for batch in train_dataloader:           # Load batch
        images_train, labels_train = batch
        optimizer.zero_grad()
        images_train = images_train.to(device, dtype=torch.float)
        labels_train = labels_train.type(torch.LongTensor)
        labels_train = labels_train.to(device)

        preds_train = model(images_train)             # Process batch

        train_loss = loss_func(preds_train, labels_train) # Calculate loss

        train_loss.backward()                 # Calculate gradients
        optimizer.step()                # Update weights
        total_train_loss += train_loss.item()

        if not validation:
            output = preds_train.argmax(dim=1)
            total_images += int(labels_train.size(0))
            total_correct += int(output.eq(labels_train).sum().item())

    # Get train loss
    train_loss = total_train_loss/len(train_dataloader) 

    # Validation
    if validation:
        with torch.no_grad():
            model.eval()
            for batch in val_dataloader:
                images_val, labels_val = batch
                images_val = images_val.to(device, dtype=torch.float)
                labels_val = labels_val.type(torch.LongTensor)
                labels_val = labels_val.to(device)

                preds_val = model(images_val)
                val_loss = loss_func(preds_val, labels_val) 
                total_val_loss += val_loss.item()

                output = preds_val.argmax(dim=1)
                total_images += int(labels_val.size(0))
                total_correct += int(output.eq(labels_val).sum().item())

        # Get val loss
        val_loss = total_val_loss/len(val_dataloader) 

    model_accuracy = total_correct / total_images

    if not validation:
        print(f"ep {epoch}, train loss: {train_loss:.2f}, Train Acc {model_accuracy*100:.2f}%")
    else:
        print(f"ep {epoch}, train loss: {train_loss:.2f}, val loss: {val_loss:.2f}, Val Acc {model_accuracy*100:.2f}%")

    # if epoch % 10 == 0:
    #     torch.save(net.state_dict(),'checkModel.pth')
    #     print("   Model saved to checkModel.pth")

    if model_accuracy > best_accuracy:
        torch.save(net.state_dict(),'savedModel.pth')
        print("\tModel saved to savedModel.pth")
        best_accuracy = model_accuracy 

    sys.stdout.flush()

# torch.save(net.state_dict(),'savedModel.pth')
# print("   Model saved to savedModel.pth")

end = time.time()
print(f"\nTotal training time: {str(datetime.timedelta(seconds=end-start))}")

# Function to test the model
def test():
    # Load the model that we saved at the end of the training loop
    model = Network()
    path = "savedModel.pth"
    model.load_state_dict(torch.load(path))
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using {device} device")
    model.to(device)
    running_accuracy = 0
    total = 0

    with torch.no_grad():
        for data in test_dataloader:
            images, outputs = data
            outputs = outputs.to(device, dtype=torch.float32)
            images = images.to(device, dtype=torch.float)
            predicted_outputs = model(images)
            _, predicted = torch.max(predicted_outputs, 1)
            total += outputs.size(0)
            running_accuracy += (predicted == outputs).sum().item()

        print('Accuracy of the model based on the test set of the inputs is: %d %%' % (100 * running_accuracy / total))

start = time.time()
test()
end = time.time()
print(f"\nTotal testing time: {str(datetime.timedelta(seconds=end-start))}")